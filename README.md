<<<<<<< HEAD
# this is based on the code from https://github.com/quanpn90/NMTGMinor.git and the code from https://github.com/huggingface/transformers


<<<<<<< HEAD
# Transformer networks for Neural Machine Translation

This is an implementation of the transformer for the paper

["Attention is all you need"](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)

It also contains the implementation of the Speech Transformer as in

["Very Deep Self-Attention Networks for End-to-End Speech Recognition"](https://arxiv.org/abs/1904.13377)

----------------------------
Requirement: (mostly recommended to use with Anaconda3 - Python3.7)

PyTorch (1.0.1 or 1.1). The code base is normally kept up-to-date with the latest version of PyTorch. 

hdf5 

apex (the C++ and CUDA extensions are not required) https://github.com/nvidia/apex

Recipe for the Speech model coming soon.

----------------------------

=======
# bert_transformer
leverage the pretrained model Bert to do nmt
>>>>>>> 01028340096eee43c63902fdd726e5dc9225184f
=======
# Init_transformer_encoder-
Initialize the encoder of Transformer with the weights of the pretrained Bert before training Transformer
>>>>>>> 35e03bd5d8351b751e29bbe63431b2b91f6d6c87
